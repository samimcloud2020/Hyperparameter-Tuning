{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#How to Grid Search Hyperparameters for Deep Learning Models in Python with Keras\n",
        "\n",
        "##How to wrap Keras models for use in scikit-learn and how to use grid search.\n",
        "\n",
        "##How to grid search common neural network parameters, such as learning rate, dropout rate, epochs, and number of neurons.\n",
        "\n",
        "##How to define your own hyperparameter tuning experiments on your own projects.\n",
        "\n",
        "How to use Keras models in scikit-learn\n",
        "\n",
        "How to use grid search in scikit-learn\n",
        "\n",
        "How to tune batch size and training epochs\n",
        "\n",
        "How to tune optimization algorithms\n",
        "\n",
        "How to tune learning rate and momentum\n",
        "\n",
        "How to tune network weight initialization\n",
        "\n",
        "How to tune activation functions\n",
        "\n",
        "How to tune dropout regularization\n",
        "\n",
        "How to tune the number of neurons in the hidden layer\n",
        "\n",
        "\n",
        "#How to Use Keras Models in scikit-learn\n",
        "Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class from the module SciKeras. You may need to run the command pip install scikeras first to install the module.\n",
        "\n",
        "To use these wrappers, you must define a function that creates and returns your Keras sequential model, then pass this function to the model argument when constructing the KerasClassifier class.\n",
        "\n",
        "For example:\n",
        "\n",
        "def create_model():\n",
        "\n",
        " ...\n",
        "\n",
        " return model\n",
        "\n",
        "##model = KerasClassifier(model=create_model)\n",
        "*************************************************************************\n",
        "\n",
        "The constructor for the KerasClassifier class can take default arguments that are passed on to the calls to model.fit(), such as the number of epochs and the batch size.\n",
        "\n",
        "For example:\n",
        "\n",
        "def create_model():\n",
        "\n",
        " ...\n",
        "\n",
        " return model\n",
        "\n",
        "##model = KerasClassifier(model=create_model, epochs=10)\n",
        "************************************************************\n",
        "\n",
        "The constructor for the KerasClassifier class can also take new arguments that can be passed to your custom create_model() function. These new arguments must also be defined in the signature of your create_model() function with default parameters.\n",
        "\n",
        "For example:\n",
        "\n",
        "def create_model(dropout_rate=0.0):\n",
        "\n",
        " ...\n",
        "\n",
        " return model\n",
        "\n",
        "##model = KerasClassifier(model=create_model, dropout_rate=0.2)"
      ],
      "metadata": {
        "id": "0D8NCjEqQXyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Use Grid Search in scikit-learn\n",
        "Grid search is a model hyperparameter optimization technique.\n",
        "\n",
        "In scikit-learn, this technique is provided in the GridSearchCV class.\n",
        "\n",
        "When constructing this class, you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.\n",
        "\n",
        "By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.\n",
        "\n",
        "By default, the grid search will only use **one thread**. By setting the **n_jobs** argument in the GridSearchCV constructor to -1, the process will **use all cores** on your machine. However, sometimes this may interfere with the main neural network training process.\n",
        "\n",
        "The GridSearchCV process will then **construct and evaluate one model for each combination of parameters.** Cross validation is used to evaluate each individual model, and the **default of 3-fold cross validation** is used, although you can override this by specifying the cv argument to the GridSearchCV constructor.\n",
        "\n",
        "Below is an example of defining a simple grid search:\n",
        "\n",
        "param_grid = dict(epochs=[10,20,30])\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "Once completed, you can access the outcome of the grid search in the result object returned from grid.fit(). The **best_score_** member provides access to the best score observed during the optimization procedure, and the **best_params_** describes the combination of parameters that achieved the best results.\n",
        "\n",
        "\n",
        "***********************************************************************\n",
        "#Problem Description\n",
        "\n",
        "Now that you know how to use Keras models with scikit-learn and how to use grid search in scikit-learn, let’s look at a bunch of examples.\n",
        "\n",
        "All examples will be demonstrated on a small standard machine learning dataset called the Pima Indians onset of diabetes classification dataset. This is a small dataset with all numerical attributes that is easy to work with.\n",
        "\n",
        "Download the dataset and place it in your currently working directly with the name pima-indians-diabetes.csv (update: download https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv).\n",
        "\n",
        "***************************************************************************\n",
        "#Note on Parallelizing Grid Search\n",
        "All examples are configured to use parallelism (n_jobs=-1).\n",
        "\n",
        "If you get an error like the one below:\n",
        "\n",
        "INFO (theano.gof.compilelock): Waiting for existing lock by process '55614' (I am process '55613')\n",
        "INFO (theano.gof.compilelock): To manually release the lock, delete ...\n",
        "Kill the process and change the code to not perform the grid search in parallel; set n_jobs=1.\n",
        "*************************************************************************\n",
        "#How to Tune Batch Size and Number of Epochs\n",
        "In this first simple example, you will look at tuning the batch size and number of epochs used when fitting the network.\n",
        "\n",
        "The **batch size** in iterative gradient descent is **the number of patterns shown to the network before the weights are updated.** It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.\n",
        "\n",
        "The **number of epochs** is the **number of times the entire training dataset is shown to the network during training.**\n",
        "\n",
        "###Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks.\n",
        "\n",
        "Here you will evaluate a suite of different mini-batch sizes from 10 to 100 in steps of 20.\n",
        "************************************************************************\n",
        "\n"
      ],
      "metadata": {
        "id": "eepZXJLeUq9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fEmLbO6ZhUQ",
        "outputId": "68a6f457-84a5-48b7-e495-4576577ccaf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0eFIyfmZ6mw",
        "outputId": "48165add-f7df-4b9f-b2a6-1109a2f9cc9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-03 05:36:01--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23278 (23K) [text/plain]\n",
            "Saving to: ‘pima-indians-diabetes.data.csv’\n",
            "\n",
            "\r          pima-indi   0%[                    ]       0  --.-KB/s               \rpima-indians-diabet 100%[===================>]  22.73K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-12-03 05:36:01 (10.2 MB/s) - ‘pima-indians-diabetes.data.csv’ saved [23278/23278]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome."
      ],
      "metadata": {
        "id": "k95fzTCNeJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JY_a4ysSY2L",
        "outputId": "23543ac2-aa14-4d9d-cb92-1e5916f1513b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            "(768, 8)\n",
            "(768,)\n",
            "Epoch 1/100\n",
            "39/39 [==============================] - 1s 2ms/step - loss: 2.0417 - accuracy: 0.4818\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 1.2407 - accuracy: 0.5208\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 1.0091 - accuracy: 0.5573\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.9013 - accuracy: 0.5547\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.8336 - accuracy: 0.5768\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.7981 - accuracy: 0.5755\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.7680 - accuracy: 0.5898\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.7462 - accuracy: 0.6250\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.7239 - accuracy: 0.6250\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.7070 - accuracy: 0.6380\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6940 - accuracy: 0.6549\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6812 - accuracy: 0.6628\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.6576\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.6654\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.6615\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.6615\n",
            "Epoch 17/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.6602\n",
            "Epoch 18/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.6680\n",
            "Epoch 19/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.6667\n",
            "Epoch 20/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.6667\n",
            "Epoch 21/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6615\n",
            "Epoch 22/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.6667\n",
            "Epoch 23/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6351 - accuracy: 0.6667\n",
            "Epoch 24/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.6693\n",
            "Epoch 25/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.6693\n",
            "Epoch 26/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.6641\n",
            "Epoch 27/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.6719\n",
            "Epoch 28/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.6641\n",
            "Epoch 29/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6219 - accuracy: 0.6771\n",
            "Epoch 30/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.6628\n",
            "Epoch 31/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6145 - accuracy: 0.6706\n",
            "Epoch 32/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.6823\n",
            "Epoch 33/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6080 - accuracy: 0.6797\n",
            "Epoch 34/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.6953\n",
            "Epoch 35/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6953\n",
            "Epoch 36/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.6010 - accuracy: 0.7005\n",
            "Epoch 37/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.6953\n",
            "Epoch 38/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5979 - accuracy: 0.7018\n",
            "Epoch 39/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5954 - accuracy: 0.6979\n",
            "Epoch 40/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5943 - accuracy: 0.6940\n",
            "Epoch 41/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.7096\n",
            "Epoch 42/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5901 - accuracy: 0.7070\n",
            "Epoch 43/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7018\n",
            "Epoch 44/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.7044\n",
            "Epoch 45/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5848 - accuracy: 0.7083\n",
            "Epoch 46/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.7122\n",
            "Epoch 47/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.7148\n",
            "Epoch 48/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5810 - accuracy: 0.7188\n",
            "Epoch 49/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7135\n",
            "Epoch 50/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5761 - accuracy: 0.7227\n",
            "Epoch 51/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7148\n",
            "Epoch 52/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.7083\n",
            "Epoch 53/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7279\n",
            "Epoch 54/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7227\n",
            "Epoch 55/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5676 - accuracy: 0.7279\n",
            "Epoch 56/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7227\n",
            "Epoch 57/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7240\n",
            "Epoch 58/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.7161\n",
            "Epoch 59/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.7201\n",
            "Epoch 60/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.7292\n",
            "Epoch 61/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5623 - accuracy: 0.7188\n",
            "Epoch 62/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.7318\n",
            "Epoch 63/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7292\n",
            "Epoch 64/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5602 - accuracy: 0.7240\n",
            "Epoch 65/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5575 - accuracy: 0.7201\n",
            "Epoch 66/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5559 - accuracy: 0.7253\n",
            "Epoch 67/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7266\n",
            "Epoch 68/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5544 - accuracy: 0.7253\n",
            "Epoch 69/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.7253\n",
            "Epoch 70/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5528 - accuracy: 0.7253\n",
            "Epoch 71/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7279\n",
            "Epoch 72/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7266\n",
            "Epoch 73/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7370\n",
            "Epoch 74/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5510 - accuracy: 0.7266\n",
            "Epoch 75/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.7318\n",
            "Epoch 76/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7370\n",
            "Epoch 77/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7422\n",
            "Epoch 78/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5467 - accuracy: 0.7344\n",
            "Epoch 79/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5455 - accuracy: 0.7318\n",
            "Epoch 80/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7370\n",
            "Epoch 81/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7396\n",
            "Epoch 82/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5431 - accuracy: 0.7409\n",
            "Epoch 83/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.7383\n",
            "Epoch 84/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7357\n",
            "Epoch 85/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5407 - accuracy: 0.7370\n",
            "Epoch 86/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5394 - accuracy: 0.7331\n",
            "Epoch 87/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5407 - accuracy: 0.7409\n",
            "Epoch 88/100\n",
            "39/39 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7409\n",
            "Epoch 89/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7409\n",
            "Epoch 90/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.7383\n",
            "Epoch 91/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7292\n",
            "Epoch 92/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7318\n",
            "Epoch 93/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7435\n",
            "Epoch 94/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.7383\n",
            "Epoch 95/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7422\n",
            "Epoch 96/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7435\n",
            "Epoch 97/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.7422\n",
            "Epoch 98/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7422\n",
            "Epoch 99/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7474\n",
            "Epoch 100/100\n",
            "39/39 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7357\n",
            "***********************************************************************\n",
            "Best: 0.704427 using {'batch_size': 20, 'epochs': 100}\n",
            "***********************************************************************\n",
            "0.658854 (0.019488) with: {'batch_size': 10, 'epochs': 10}\n",
            "0.692708 (0.016367) with: {'batch_size': 10, 'epochs': 50}\n",
            "0.670573 (0.012890) with: {'batch_size': 10, 'epochs': 100}\n",
            "0.578125 (0.013902) with: {'batch_size': 20, 'epochs': 10}\n",
            "0.645833 (0.027866) with: {'batch_size': 20, 'epochs': 50}\n",
            "0.704427 (0.011201) with: {'batch_size': 20, 'epochs': 100}\n",
            "0.553385 (0.041626) with: {'batch_size': 40, 'epochs': 10}\n",
            "0.670573 (0.036272) with: {'batch_size': 40, 'epochs': 50}\n",
            "0.684896 (0.019225) with: {'batch_size': 40, 'epochs': 100}\n",
            "0.550781 (0.027251) with: {'batch_size': 60, 'epochs': 10}\n",
            "0.664062 (0.005524) with: {'batch_size': 60, 'epochs': 50}\n",
            "0.656250 (0.019401) with: {'batch_size': 60, 'epochs': 100}\n",
            "0.473958 (0.086547) with: {'batch_size': 80, 'epochs': 10}\n",
            "0.601562 (0.022097) with: {'batch_size': 80, 'epochs': 50}\n",
            "0.682292 (0.034104) with: {'batch_size': 80, 'epochs': 100}\n",
            "0.540365 (0.093333) with: {'batch_size': 100, 'epochs': 10}\n",
            "0.662760 (0.015733) with: {'batch_size': 100, 'epochs': 50}\n",
            "0.632812 (0.026107) with: {'batch_size': 100, 'epochs': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##You can see that the batch size of 10 and 100 epochs achieved the best result of about 70% accuracy."
      ],
      "metadata": {
        "id": "_P-tLPd-eNKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune the Training Optimization Algorithm\n",
        "Keras offers a suite of different state-of-the-art optimization algorithms.\n",
        "\n",
        "In this example, you will tune the optimization algorithm used to train the network, each with default parameters.\n",
        "\n",
        "This is an odd example because often, you will choose one approach a priori and instead focus on tuning its parameters on your problem.\n",
        "\n",
        "Here, you will evaluate the suite of optimization algorithms supported by the Keras API."
      ],
      "metadata": {
        "id": "JO6dF89LKp1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# return model without compile\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "\n",
        "# summarize results\n",
        "print(\"**********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rfqu2E2eUec",
        "outputId": "b3784593-c878-448f-f37b-f62df50eee4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 30.0094\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 5.0262\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.7248\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.4871\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.2396\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.0263\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.8492\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.7171\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.5101\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.4047\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.3030\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.1738\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0280\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9542\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.9067\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8352\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8046\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7890\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7628\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7146\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7197\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7006\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6883\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6759\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6580\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6815\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6464\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6536\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6298\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6268\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6209\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6608\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6142\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5984\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5904\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6094\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6276\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6036\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5802\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5911\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5836\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5813\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5837\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5801\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5866\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5864\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5706\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5649\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5835\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5656\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5598\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5515\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5575\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5715\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5618\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5454\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5533\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5657\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5535\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5680\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5304\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5381\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5433\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5570\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5564\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5817\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5345\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5422\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5464\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5368\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5570\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5377\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5288\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5610\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5408\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5568\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5359\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5472\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5663\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5446\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5341\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5692\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5620\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5430\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5550\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5246\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5192\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5180\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5219\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5371\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5718\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5348\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5250\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5231\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5115\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5183\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5102\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5094\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5237\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5370\n",
            "**********************************************************************\n",
            "Best: 0.690104 using {'optimizer': 'Adam'}\n",
            "***********************************************************************\n",
            "0.638021 (0.031948) with: {'optimizer': 'SGD'}\n",
            "0.664062 (0.033603) with: {'optimizer': 'RMSprop'}\n",
            "0.562500 (0.013902) with: {'optimizer': 'Adagrad'}\n",
            "0.501302 (0.110132) with: {'optimizer': 'Adadelta'}\n",
            "0.690104 (0.003683) with: {'optimizer': 'Adam'}\n",
            "0.657552 (0.013279) with: {'optimizer': 'Adamax'}\n",
            "0.652344 (0.036782) with: {'optimizer': 'Nadam'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The KerasClassifier wrapper will not compile your model again if the model is already compiled. Hence the other way to run GridSearchCV is to set the optimizer as an argument to the create_model() function, which returns an appropriately compiled model like the following:"
      ],
      "metadata": {
        "id": "kmUetemwOF6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the batch size and epochs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='adam'):                  #<-----------Taken optimizer as Adam\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(model__optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdbU0R5FOIDZ",
        "outputId": "5bf44d88-b453-48f6-9b03-7e57a27c32bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 3ms/step - loss: 5.3955 - accuracy: 0.5768\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 2.2352 - accuracy: 0.6497\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.7388 - accuracy: 0.6445\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.5266 - accuracy: 0.6237\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.4000 - accuracy: 0.6107\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.3578 - accuracy: 0.6211\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2555 - accuracy: 0.6302\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.2451 - accuracy: 0.6224\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.1399 - accuracy: 0.6458\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.1485 - accuracy: 0.6367\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0726 - accuracy: 0.6510\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0486 - accuracy: 0.6328\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.0136 - accuracy: 0.6354\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.9578 - accuracy: 0.6419\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.9584 - accuracy: 0.6393\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.9274 - accuracy: 0.6576\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.9003 - accuracy: 0.6393\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8771 - accuracy: 0.6380\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.8403 - accuracy: 0.6458\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.8007 - accuracy: 0.6536\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8194 - accuracy: 0.6497\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.7711 - accuracy: 0.6510\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.7497 - accuracy: 0.6536\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.7968 - accuracy: 0.6576\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 1s 8ms/step - loss: 0.8123 - accuracy: 0.6419\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 1s 8ms/step - loss: 0.7952 - accuracy: 0.6523\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 6ms/step - loss: 0.7223 - accuracy: 0.6562\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 1s 8ms/step - loss: 0.7288 - accuracy: 0.6693\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.6589\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7370 - accuracy: 0.6615\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7330 - accuracy: 0.6536\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7652 - accuracy: 0.6497\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7159 - accuracy: 0.6784\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.6797\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.6888\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7099 - accuracy: 0.6576\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.6615\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.6901\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.6901\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6901\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.6862\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.6693\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6624 - accuracy: 0.6875\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.6771\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.6927\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6784\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6719\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6914\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6875\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6836\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6203 - accuracy: 0.6979\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6205 - accuracy: 0.7135\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.7031\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.6914\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.6953\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.7227\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.7096\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.6875\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.6901\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6966\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.7005\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6927\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.7083\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6053 - accuracy: 0.6992\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.7070\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6602 - accuracy: 0.6940\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.7148\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5809 - accuracy: 0.7174\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.6966\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.7096\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7188\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.7057\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.7109\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.7135\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.7096\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.7214\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7292\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7161\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6164 - accuracy: 0.7005\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.7344\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7318\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.7096\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.6849\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.7318\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7292\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7409\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7318\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7161\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7214\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7266\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7070\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.7253\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7318\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.7331\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7617\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7487\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.7409\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7331\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7513\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5847 - accuracy: 0.7214\n",
            "***********************************************************************\n",
            "Best: 0.727865 using {'model__optimizer': 'Adam'}\n",
            "***********************************************************************\n",
            "0.660156 (0.012758) with: {'model__optimizer': 'SGD'}\n",
            "0.679688 (0.033299) with: {'model__optimizer': 'RMSprop'}\n",
            "0.470052 (0.074139) with: {'model__optimizer': 'Adagrad'}\n",
            "0.472656 (0.083840) with: {'model__optimizer': 'Adadelta'}\n",
            "0.727865 (0.023939) with: {'model__optimizer': 'Adam'}\n",
            "0.674479 (0.027498) with: {'model__optimizer': 'Adamax'}\n",
            "0.697917 (0.023073) with: {'model__optimizer': 'Nadam'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note that in the above, you have the **prefix model__** in the parameter dictionary param_grid. This is required for the KerasClassifier in the SciKeras module to make clear that the parameter needs to route into the create_model() function as arguments, rather than some parameter to set up in compile() or fit(). See also the routed parameter section of SciKeras documentation."
      ],
      "metadata": {
        "id": "__ztUd4hQ2nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The results suggest that the ADAM optimization algorithm is the best with a score of about 70% accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "vBjFLXI9RZtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune Learning Rate and Momentum\n",
        "It is common to pre-select an optimization algorithm to train your network and tune its parameters.\n",
        "\n",
        "By far, the most common optimization algorithm is **plain old Stochastic Gradient Descent (SGD)** because it is so well understood. In this example, you will look at optimizing the **SGD learning rate and momentum parameters.**\n",
        "\n",
        "The **learning rate** controls how much to update the weight at the end of each batch, and the **momentum** controls how much to let the previous update influence the current weight update.\n",
        "\n",
        "You will try a suite of small standard learning rates and momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice). In Keras, the way to set the learning rate and momentum is the following:\n",
        "\n",
        "...\n",
        "\n",
        "###optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.2)\n",
        "\n",
        "In the SciKeras wrapper, you will route the parameters to the optimizer with the **prefix optimizer__.**\n",
        "\n",
        "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size), and the number of epochs"
      ],
      "metadata": {
        "id": "8EwLYe8wRsBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the learning rate and momentum\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", optimizer=\"SGD\", epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0xmsM9yRCsq",
        "outputId": "f6b39886-eecc-43e8-eca4-64d702150be0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 3ms/step - loss: 2.3505\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7006\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6644\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6526\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6493\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6343\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6369\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6386\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6285\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6220\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6235\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6162\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6219\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6183\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6115\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6206\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6117\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6161\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6157\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6014\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6121\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6106\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5990\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6037\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6047\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6005\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5931\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6031\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5964\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5934\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5996\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5976\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5935\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5875\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5873\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5989\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5941\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5925\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5887\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5778\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5834\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5854\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5823\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5852\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5923\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5849\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5829\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5820\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5827\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5817\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5797\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5768\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5769\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5812\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5815\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5761\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5830\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5742\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5730\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5794\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5680\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5735\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5832\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5821\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5720\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5843\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5712\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5803\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5765\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5757\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5761\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5747\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5794\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5730\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5676\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5753\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5807\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5647\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5819\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5911\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5724\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5793\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5845\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5741\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5773\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5723\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5690\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5697\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5652\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5651\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5746\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5678\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5703\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5760\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5619\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5647\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5649\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5636\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5648\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5691\n",
            "***********************************************************************\n",
            "Best: 0.687500 using {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
            "***********************************************************************\n",
            "0.635417 (0.059270) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
            "0.664062 (0.050126) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.2}\n",
            "0.678385 (0.023073) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.4}\n",
            "0.678385 (0.021236) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.6}\n",
            "0.687500 (0.003189) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
            "0.662760 (0.014382) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.9}\n",
            "0.665365 (0.013279) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
            "0.649740 (0.018414) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.2}\n",
            "0.644531 (0.011049) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.4}\n",
            "0.649740 (0.003683) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.6}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.8}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.9}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.0}\n",
            "0.649740 (0.003683) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.2}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.4}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.6}\n",
            "0.652344 (0.003189) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.8}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.9}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.0}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.2}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.4}\n",
            "0.649740 (0.003683) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.6}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.8}\n",
            "0.652344 (0.003189) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.9}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.0}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.2}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.4}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.6}\n",
            "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.8}\n",
            "0.649740 (0.003683) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##You can see that SGD is not very good on this problem; nevertheless, the best results were achieved using a learning rate of 0.001 and a momentum of 0.8 with an accuracy of about 68%."
      ],
      "metadata": {
        "id": "AVsOXibCYzV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune Network Weight Initialization\n",
        "Neural network weight initialization used to be simple: use small random values.\n",
        "\n",
        "Now there is a suite of different techniques to choose from. Keras provides a laundry list.\n",
        "\n",
        "In this example, you will look at tuning the selection of network weight initialization by evaluating all the available techniques.\n",
        "\n",
        "You will use the same weight initialization method on each layer.\n",
        "\n",
        " Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer.\n",
        "\n",
        " In the example below, you will use a rectifier for the hidden layer. Use sigmoid for the output layer because the predictions are binary. The weight initialization is now an argument to create_model() function, where you need to use the model__ prefix to ask the KerasClassifier to route the parameter to the model creation function."
      ],
      "metadata": {
        "id": "VYA3YT8XY972"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the weight initialization\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "# Function to create model, required for KerasClassifier\n",
        "\n",
        "def create_model(init_mode='uniform'):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), kernel_initializer=init_mode, activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']   #initialization mode\n",
        "param_grid = dict(model__init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swaXhTrfZbdv",
        "outputId": "e729144e-956a-4e25-b3cf-b601294b60c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6471\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.6484\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6320 - accuracy: 0.6628\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6321 - accuracy: 0.6615\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6271 - accuracy: 0.6693\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6849\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6849\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.6836\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.6927\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6020 - accuracy: 0.7018\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6008 - accuracy: 0.6914\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6015 - accuracy: 0.7005\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.6940\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.6875\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5912 - accuracy: 0.6979\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5894 - accuracy: 0.6914\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.6914\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5928 - accuracy: 0.6940\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5846 - accuracy: 0.6940\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5826 - accuracy: 0.6979\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5872 - accuracy: 0.6966\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.7109\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7188\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.7174\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.7188\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.7135\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5818 - accuracy: 0.7148\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.7318\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5678 - accuracy: 0.7214\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.7266\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7253\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7292\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7214\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7240\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7240\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.7370\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7370\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7357\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7344\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7279\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7253\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7253\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5453 - accuracy: 0.7344\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7318\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7370\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7344\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7461\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.7357\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7409\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7383\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7461\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7318\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7396\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7422\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7383\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7370\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7383\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7357\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7409\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7422\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7682\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7435\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7422\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7331\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7396\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7461\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7396\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7331\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7474\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7357\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7305\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7617\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7513\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7435\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7435\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7461\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7409\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7448\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.7305\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7474\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7487\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7396\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7513\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7487\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7487\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7461\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5145 - accuracy: 0.7448\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.7552\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7487\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7539\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5162 - accuracy: 0.7461\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7383\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7539\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5130 - accuracy: 0.7487\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.7604\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7526\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7513\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7500\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5145 - accuracy: 0.7487\n",
            "***********************************************************************\n",
            "Best: 0.730469 using {'model__init_mode': 'uniform'}\n",
            "***********************************************************************\n",
            "0.730469 (0.026107) with: {'model__init_mode': 'uniform'}\n",
            "0.694010 (0.010253) with: {'model__init_mode': 'lecun_uniform'}\n",
            "0.720052 (0.031948) with: {'model__init_mode': 'normal'}\n",
            "0.651042 (0.001841) with: {'model__init_mode': 'zero'}\n",
            "0.678385 (0.028940) with: {'model__init_mode': 'glorot_normal'}\n",
            "0.665365 (0.050664) with: {'model__init_mode': 'glorot_uniform'}\n",
            "0.696615 (0.018688) with: {'model__init_mode': 'he_normal'}\n",
            "0.699219 (0.006379) with: {'model__init_mode': 'he_uniform'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We can see that the best results were achieved with a uniform weight initialization scheme achieving a performance of about 72%."
      ],
      "metadata": {
        "id": "Z2YwheKeaC77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune the Neuron Activation Function\n",
        "The activation function controls the non-linearity of individual neurons and when to fire.\n",
        "\n",
        "Generally, the rectifier activation function is the most popular. However, it used to be the **sigmoid and the tanh functions,** and these functions may still be more suitable for different problems.\n",
        "\n",
        "In this example, you will evaluate the suite of different activation functions available in Keras. You will only use these functions in the hidden layer, as a **sigmoid activation function is required in the output for the binary classification problem.** Similar to the previous example, this is an argument to the create_model() function, and you will use the model__ prefix for the GridSearchCV parameter grid.\n",
        "\n",
        "Generally, it is a good idea to prepare data to the range of the different transfer functions, which you will not do in this case."
      ],
      "metadata": {
        "id": "POII84x8cBiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the activation function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(activation='relu'):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_shape=(8,), kernel_initializer='uniform', activation=activation))\n",
        "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=1)\n",
        "# define the grid search parameters\n",
        "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "param_grid = dict(model__activation=activation)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IETUWFCJcd5e",
        "outputId": "a9e76853-0a9d-4295-d154-9d180b89a3a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 0.6613 - accuracy: 0.6393\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.6654\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6602\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.6589\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.6654\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.6602\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.6797\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.6823\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6143 - accuracy: 0.6823\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.6927\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6784\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6862\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6116 - accuracy: 0.6797\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.6888\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6094 - accuracy: 0.6693\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6053 - accuracy: 0.6940\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.6875\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.6888\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.6862\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6927\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5986 - accuracy: 0.6992\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6888\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.6992\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6003 - accuracy: 0.6979\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6940\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5952 - accuracy: 0.7044\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.6979\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.7044\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.6979\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.6992\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5962 - accuracy: 0.6888\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5931 - accuracy: 0.7031\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.7005\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7161\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5850 - accuracy: 0.7109\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5884 - accuracy: 0.7044\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5844 - accuracy: 0.7031\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.7122\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.7161\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5843 - accuracy: 0.6966\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5857 - accuracy: 0.7044\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5848 - accuracy: 0.7174\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5815 - accuracy: 0.7096\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7148\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.6966\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5812 - accuracy: 0.7057\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5765 - accuracy: 0.7214\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5747 - accuracy: 0.7135\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5886 - accuracy: 0.7031\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.7174\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.7201\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5673 - accuracy: 0.7214\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5746 - accuracy: 0.7057\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.7188\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.7227\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.7148\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6888\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5709 - accuracy: 0.7188\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7109\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.7201\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.7083\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7253\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5701 - accuracy: 0.7109\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5723 - accuracy: 0.7174\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7292\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.7148\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7318\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7344\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.7266\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7279\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7292\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5623 - accuracy: 0.7266\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7331\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.7253\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5581 - accuracy: 0.7305\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7214\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7253\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7161\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5609 - accuracy: 0.7240\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7161\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7357\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7214\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7122\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7344\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5570 - accuracy: 0.7448\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.7266\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7331\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5538 - accuracy: 0.7083\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.7318\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7227\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7227\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7331\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7344\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7409\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7253\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5555 - accuracy: 0.7227\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7357\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7370\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.7344\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7370\n",
            "***********************************************************************\n",
            "Best: 0.723958 using {'model__activation': 'linear'}\n",
            "***********************************************************************\n",
            "0.652344 (0.003189) with: {'model__activation': 'softmax'}\n",
            "0.721354 (0.022402) with: {'model__activation': 'softplus'}\n",
            "0.658854 (0.025976) with: {'model__activation': 'softsign'}\n",
            "0.718750 (0.022999) with: {'model__activation': 'relu'}\n",
            "0.669271 (0.007366) with: {'model__activation': 'tanh'}\n",
            "0.696615 (0.025780) with: {'model__activation': 'sigmoid'}\n",
            "0.694010 (0.043067) with: {'model__activation': 'hard_sigmoid'}\n",
            "0.723958 (0.015733) with: {'model__activation': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Surprisingly (to me at least), the “linear” activation function achieved the best results with an accuracy of about 71%."
      ],
      "metadata": {
        "id": "rirwK2OcfnkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune Dropout Regularization\n",
        "In this example, you will look at tuning the **dropout rate** for regularization in an effort **to limit overfitting** and improve the model’s ability to generalize.\n",
        "\n",
        "For the best results, dropout is best combined with a weight constraint such as the max norm constraint.\n",
        "\n",
        "For more on using dropout in deep learning models with Keras see the post:\n",
        "\n",
        "###Dropout Regularization in Deep Learning Models With Keras\n",
        "This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between 0.0 and 0.9 (1.0 does not make sense) and maxnorm weight constraint values between 0 and 5."
      ],
      "metadata": {
        "id": "O0S29ia1ftY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the dropout rate\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(dropout_rate, weight_constraint):     # droprate and weight_constraint\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(12, input_shape=(8,), kernel_initializer='uniform', activation='linear', kernel_constraint=MaxNorm(weight_constraint)))\n",
        " model.add(Dropout(dropout_rate))\n",
        " model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        " # Compile model\n",
        " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " return model\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "print(dataset.dtype, dataset.shape)\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
        "#param_grid = dict(model__dropout_rate=dropout_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fYh0a-Zgm33",
        "outputId": "069098db-19f3-4d66-a7a0-f539676e7bfa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64 (768, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 0.6745 - accuracy: 0.6354\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.6549\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6523\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6381 - accuracy: 0.6523\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6319 - accuracy: 0.6680\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.6654\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.6823\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.6810\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.6810\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6888\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6797\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6810\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.6875\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6064 - accuracy: 0.6927\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.6732\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6049 - accuracy: 0.6901\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6024 - accuracy: 0.6901\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6156 - accuracy: 0.6875\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.6875\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6966\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6992\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.6927\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7018\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.6979\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.6940\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5950 - accuracy: 0.7044\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5909 - accuracy: 0.7018\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.7005\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.6979\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5923 - accuracy: 0.7005\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.6927\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5931 - accuracy: 0.7018\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5881 - accuracy: 0.7005\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5844 - accuracy: 0.7148\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.7096\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5881 - accuracy: 0.7070\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5845 - accuracy: 0.7057\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5822 - accuracy: 0.7122\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.7148\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.6979\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5858 - accuracy: 0.7083\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5848 - accuracy: 0.7096\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7096\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.7122\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.6953\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.7044\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5762 - accuracy: 0.7201\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.7096\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.6953\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5780 - accuracy: 0.7188\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7201\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7214\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.7031\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7214\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5700 - accuracy: 0.7201\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7122\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5922 - accuracy: 0.6914\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7188\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7096\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7188\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7083\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7279\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7122\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.7174\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7292\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5748 - accuracy: 0.7122\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7305\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7357\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.7227\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7279\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5629 - accuracy: 0.7266\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7240\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.7357\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7279\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7292\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7266\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7240\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7174\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7292\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.7188\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7318\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7201\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5593 - accuracy: 0.7122\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7331\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.7474\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5527 - accuracy: 0.7279\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7344\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7109\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7331\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7253\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5574 - accuracy: 0.7201\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7331\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7357\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7409\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.7253\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.7227\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7344\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5490 - accuracy: 0.7370\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7357\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.7370\n",
            "***********************************************************************\n",
            "Best: 0.730469 using {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
            "***********************************************************************\n",
            "0.705729 (0.006639) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
            "0.704427 (0.009744) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
            "0.730469 (0.015947) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
            "0.713542 (0.009744) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
            "0.729167 (0.012075) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
            "0.699219 (0.011500) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
            "0.696615 (0.010253) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
            "0.722656 (0.008438) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
            "0.692708 (0.004872) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
            "0.727865 (0.011201) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
            "0.692708 (0.003683) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
            "0.699219 (0.019918) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
            "0.714844 (0.011049) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
            "0.710938 (0.008438) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
            "0.712240 (0.008027) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
            "0.700521 (0.010253) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
            "0.725260 (0.016367) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
            "0.699219 (0.006379) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
            "0.716146 (0.010253) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
            "0.686198 (0.018136) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
            "0.692708 (0.011201) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
            "0.713542 (0.009744) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
            "0.695312 (0.020915) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
            "0.705729 (0.007366) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
            "0.690104 (0.018414) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
            "0.716146 (0.008027) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
            "0.707031 (0.013902) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
            "0.707031 (0.013902) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
            "0.704427 (0.018414) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
            "0.707031 (0.011500) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
            "0.705729 (0.012075) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
            "0.701823 (0.014382) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
            "0.713542 (0.006639) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
            "0.696615 (0.007366) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
            "0.694010 (0.012075) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
            "0.710938 (0.003189) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
            "0.703125 (0.008438) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
            "0.704427 (0.012890) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
            "0.692708 (0.011201) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
            "0.691406 (0.013902) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
            "0.695312 (0.011500) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
            "0.691406 (0.014616) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
            "0.695312 (0.008438) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
            "0.688802 (0.001841) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
            "0.679688 (0.009568) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
            "0.673177 (0.012075) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
            "0.666667 (0.001841) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
            "0.667969 (0.006379) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
            "0.670573 (0.015733) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
            "0.673177 (0.008027) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the dropout rate of 0% and the MaxNorm weight constraint of\n",
        "3 resulted in the best accuracy of about 77%. You may notice some of the result is nan. Probably it is due to the issue that the input is not normalized and you may run into a degenerated model by chance."
      ],
      "metadata": {
        "id": "_i6PE4zsnqS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to Tune the Number of Neurons in the Hidden Layer\n",
        "The number of neurons in a layer is an important parameter to tune. Generally the **number of neurons** in a layer controls the representational capacity of the network, at least at that point in the topology.\n",
        "\n",
        "Also, generally, a large enough single layer network can approximate any other neural network, at least in theory.\n",
        "\n",
        "In this example, we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5.\n",
        "\n",
        "A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons."
      ],
      "metadata": {
        "id": "F62ZvJlKnt-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn to grid search the number of neurons\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(neurons):             #neurons\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(neurons, input_shape=(8,), kernel_initializer='uniform', activation='linear', kernel_constraint=MaxNorm(4)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataset = np.loadtxt(\"/content/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(model__neurons=neurons)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"***********************************************************************\")\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "print(\"***********************************************************************\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdmEuwm2oZqM",
        "outputId": "8aed0c89-34a5-40da-e9f9-59b85774f42e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 1s 2ms/step - loss: 0.6796 - accuracy: 0.6341\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.6589\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.6549\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.6706\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6275 - accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.6901\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.6719\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.6602\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.6875\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.6797\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.6862\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.6901\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6145 - accuracy: 0.6836\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.6667\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6115 - accuracy: 0.6849\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.6927\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6219 - accuracy: 0.6784\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6140 - accuracy: 0.6836\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.6888\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6040 - accuracy: 0.7005\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6087 - accuracy: 0.6771\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5968 - accuracy: 0.6927\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6066 - accuracy: 0.6901\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6064 - accuracy: 0.6836\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.6966\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.7031\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6020 - accuracy: 0.7031\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5964 - accuracy: 0.7070\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5988 - accuracy: 0.6979\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6004 - accuracy: 0.6927\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5946 - accuracy: 0.6888\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.6966\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5893 - accuracy: 0.7122\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5861 - accuracy: 0.7083\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5907 - accuracy: 0.7070\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5889 - accuracy: 0.6940\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.7018\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.7031\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7070\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.6992\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5819 - accuracy: 0.7148\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.7070\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5820 - accuracy: 0.7174\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5858 - accuracy: 0.6927\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.6979\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7122\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7018\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5926 - accuracy: 0.6927\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.7135\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7135\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.7096\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7018\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.7070\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.7109\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7135\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.6888\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7057\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.7070\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7031\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7122\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7305\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7083\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7083\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7161\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.7109\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.7214\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7266\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5736 - accuracy: 0.7122\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7161\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.7201\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.7096\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.7240\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7214\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7253\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7253\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7005\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7174\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7214\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.7135\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5600 - accuracy: 0.7266\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5652 - accuracy: 0.7227\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7070\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.7161\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7344\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7240\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.7292\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7109\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7305\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5585 - accuracy: 0.7135\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.7188\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7188\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.7318\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.7266\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7161\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7188\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.7253\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.7396\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.7227\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.7253\n",
            "***********************************************************************\n",
            "Best: 0.723958 using {'model__neurons': 30}\n",
            "***********************************************************************\n",
            "0.691406 (0.019137) with: {'model__neurons': 1}\n",
            "0.704427 (0.006639) with: {'model__neurons': 5}\n",
            "0.717448 (0.009744) with: {'model__neurons': 10}\n",
            "0.716146 (0.014382) with: {'model__neurons': 15}\n",
            "0.709635 (0.012075) with: {'model__neurons': 20}\n",
            "0.712240 (0.012890) with: {'model__neurons': 25}\n",
            "0.723958 (0.012075) with: {'model__neurons': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We can see that the best results were achieved with a network with 30 neurons in the hidden layer with an accuracy of about 73%."
      ],
      "metadata": {
        "id": "qoep-D92rIcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tips for Hyperparameter Optimization\n",
        "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
        "\n",
        "**k-fold Cross Validation.** You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
        "\n",
        "\n",
        "**Review the Whole Grid.** Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
        "Parallelize. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of AWS instances.\n",
        "\n",
        "**Use a Sample of Your Dataset.** Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
        "Start with Coarse Grids. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
        "\n",
        "**Do not Transfer Results.** Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
        "\n",
        "Reproducibility is a Problem. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post."
      ],
      "metadata": {
        "id": "NZF92Il3rT8l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUa68CtcrmzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}